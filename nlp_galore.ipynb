{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ihOOiWqSuGFjOI9N-iUCCHfMkZO9yl5R",
      "authorship_tag": "ABX9TyO3bEAO0dVLKacsQttdJG4j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamaskecskemeti/financial_nlp/blob/main/nlp_galore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iikI2Xf7KPMt",
        "outputId": "6095b810-d184-440e-b065-833dca9814a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.40.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.19.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.19.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.16.6)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: nvitop in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: lion-pytorch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.7.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.43.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.2.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.4.2)\n",
            "Requirement already satisfied: galore-torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (3.9.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft->-r requirements.txt (line 5)) (0.29.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (3.1.43)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (3.20.3)\n",
            "Requirement already satisfied: nvidia-ml-py<12.536.0a0,>=11.450.51 in /usr/local/lib/python3.10/dist-packages (from nvitop->-r requirements.txt (line 8)) (12.535.161)\n",
            "Requirement already satisfied: cachetools>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from nvitop->-r requirements.txt (line 8)) (5.3.3)\n",
            "Requirement already satisfied: termcolor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nvitop->-r requirements.txt (line 8)) (2.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (3.4.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6)) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 4)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 4)) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6)) (5.0.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# install required packages written in requirements\n",
        "!pip install -r requirements.txt\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import GPT2LMHeadModel, AutoTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "ylH1BMtqLSZN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EOxeW6onLB_X",
        "outputId": "b24dd46f-62df-4242-d01b-2ae24a423d4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_from_input(tokenizer, model, input_text):\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  out = model.generate(input_ids,\n",
        "                     max_new_tokens=100,\n",
        "                     num_beams=5,\n",
        "                     no_repeat_ngram_size=4,\n",
        "                     top_k=50,\n",
        "                     do_sample=True,\n",
        "                     top_p=0.9,\n",
        "                     temperature=1,\n",
        "                     early_stopping=True,\n",
        "                     pad_token_id=tokenizer.eos_token_id).to(device)\n",
        "\n",
        "  out_text = list(map(tokenizer.decode, out))[0]\n",
        "\n",
        "  return out_text"
      ],
      "metadata": {
        "id": "mVbXbGK5LCTj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rouge scores for a reference/generated sentence pair\n",
        "# source google seq2seq source code.\n",
        "\n",
        "# supporting function\n",
        "def _split_into_words(sentences):\n",
        "  \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
        "  return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
        "\n",
        "# supporting function\n",
        "def _get_word_ngrams(n, sentences):\n",
        "  \"\"\"Calculates word n-grams for multiple sentences.\n",
        "  \"\"\"\n",
        "  assert len(sentences) > 0\n",
        "  assert n > 0\n",
        "\n",
        "  words = _split_into_words(sentences)\n",
        "  return _get_ngrams(n, words)\n",
        "\n",
        "# supporting function\n",
        "def _get_ngrams(n, text):\n",
        "  \"\"\"Calcualtes n-grams.\n",
        "  Args:\n",
        "    n: which n-grams to calculate\n",
        "    text: An array of tokens\n",
        "  Returns:\n",
        "    A set of n-grams\n",
        "  \"\"\"\n",
        "  ngram_set = set()\n",
        "  text_length = len(text)\n",
        "  max_index_ngram_start = text_length - n\n",
        "  for i in range(max_index_ngram_start + 1):\n",
        "    ngram_set.add(tuple(text[i:i + n]))\n",
        "  return ngram_set\n",
        "\n",
        "def rouge_n(reference_sentences, evaluated_sentences, n=2):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    evaluated_sentences: The sentences that have been picked by the summarizer\n",
        "    reference_sentences: The sentences from the referene set\n",
        "    n: Size of ngram.  Defaults to 2.\n",
        "  Returns:\n",
        "    recall rouge score(float)\n",
        "  Raises:\n",
        "    ValueError: raises exception if a param has len <= 0\n",
        "  \"\"\"\n",
        "  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
        "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
        "\n",
        "  evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
        "  reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
        "  reference_count = len(reference_ngrams)\n",
        "  evaluated_count = len(evaluated_ngrams)\n",
        "\n",
        "  # gets the overlapping ngrams between evaluated and reference\n",
        "  overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
        "  overlapping_count = len(overlapping_ngrams)\n",
        "\n",
        "  # handle edge case. This isn't mathematically correct, but it's good enough\n",
        "  if evaluated_count == 0:\n",
        "    precision = 0.0\n",
        "  else:\n",
        "    precision = overlapping_count / evaluated_count\n",
        "\n",
        "  if reference_count == 0:\n",
        "    recall = 0.0\n",
        "  else:\n",
        "    recall = overlapping_count / reference_count\n",
        "\n",
        "  f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
        "\n",
        "  # just returning recall count in rouge, useful for our purpose\n",
        "  return recall"
      ],
      "metadata": {
        "id": "hcq-3-EuYEJZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some text to test the model\n",
        "text = Path(\"generate_text_en.txt\").read_text()"
      ],
      "metadata": {
        "id": "N9SnGfsZLxU2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\").to(device)"
      ],
      "metadata": {
        "id": "dX8uGGq4L1SF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cbabd24-bbd7-4151-8452-17e937cfa952"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text_from_input(tokenizer, model, text)\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "9DibRZqAL_sp",
        "outputId": "df495cb5-250d-40c7-ca35-788ed5c41a08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One of the biggest names in Silicon Valley is placing a moonshot bet on bitcoin BTCUSD, +0.72% . \\nFounders Fund, the venture-capital firm co-founded by Peter Thiel, has amassed hundreds of millions of dollars of the volatile cryptocurrency, people familiar with the matter said. The fund has invested in the cryptocurrency for more than a decade, according to a report by the Wall Street Journal. The fund also invested in a number of other cryptocurrencies, including Bitcoin, Ethereum, Litecoin, and Ripple.\\nThe fund is also investing in cryptocurrency exchanges, such as Bitfinex, Coinbase, and CoinMarketCap, as well as cryptocurrency brokers, such as CoinMarketCap.com. The fund is also'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the reference text is used to evaluate the generated text\n",
        "ref_text = Path(\"reference_text_en.txt\").read_text()"
      ],
      "metadata": {
        "id": "KYfGC2-xMGzV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text, generated_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyP_KGwuY_ij",
        "outputId": "865af1ab-9f2b-4a25-9cfc-7fcf3e47e5cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6872852233676976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArmbLOz-pp1M",
        "outputId": "f0ccf070-76bd-459e-9832-de570119edf9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BloomForCausalLM(\n",
            "  (transformer): BloomModel(\n",
            "    (word_embeddings): Embedding(250880, 1024)\n",
            "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (h): ModuleList(\n",
            "      (0-23): 24 x BloomBlock(\n",
            "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention): BloomAttention(\n",
            "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): BloomMLP(\n",
            "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (gelu_impl): BloomGelu()\n",
            "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Conv1D\n",
        "\n",
        "def get_specific_layer_names(model):\n",
        "    # Create a list to store the layer names\n",
        "    layer_names = []\n",
        "\n",
        "    # Recursively visit all modules and submodules\n",
        "    for name, module in model.named_modules():\n",
        "        # Check if the module is an instance of the specified layers\n",
        "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
        "            # model name parsing\n",
        "\n",
        "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
        "\n",
        "    return layer_names\n",
        "\n",
        "list(set(get_specific_layer_names(model)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6EUiKEVqqhI",
        "outputId": "5fc965ac-8995-41b9-d669-20bf5a95629d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', 'query_key_value', 'dense_4h_to_h', 'dense', 'dense_h_to_4h']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "  dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer):\n",
        "  data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "  return data_collator\n",
        "\n",
        "def train(input_path,\n",
        "          model_name,\n",
        "          output_path,\n",
        "          learning_rate,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs):\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(input_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_path)\n",
        "  # model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "  model.save_pretrained(f\"pre_finetune_{output_path}\")\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_path,\n",
        "          learning_rate=learning_rate,\n",
        "          overwrite_output_dir=False,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "          optim=\"galore_adamw_8bit_layerwise\",\n",
        "          optim_target_modules=[\"attn\", \"mlp\"]\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "metadata": {
        "id": "6p5ogxY5MNoM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"train_text_en.txt\"\n",
        "\n",
        "learning_rates = [1e-05, 2e-05, 3e-5]\n",
        "batch_sizes = [4, 8]\n",
        "combinations = [(lr, bs) for lr in learning_rates for bs in batch_sizes]\n",
        "\n",
        "for lr, bs in combinations:\n",
        "  output_path = f\"result_en_{lr}_{bs}\"\n",
        "  train(\n",
        "    input_path=input_path,\n",
        "    model_name=\"bigscience/bloom-560m\",\n",
        "    output_path=output_path,\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=bs,\n",
        "    num_train_epochs=4\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6W_t6VuPPJod",
        "outputId": "63bcc386-7997-4fb8-9f81-c77904727de6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "Activated GaLoRE fine-tuning, depending on your model size and hardware, the training might take a while before starting. Please be patient !\n",
            "transformer.h.0.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.0.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.1.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.1.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.2.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.2.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.3.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.3.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.4.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.4.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.5.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.5.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.6.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.6.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.7.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.7.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.8.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.8.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.9.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.9.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.10.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.10.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.11.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.11.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.12.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.12.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.13.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.13.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.14.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.14.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.15.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.15.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.16.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.16.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.17.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.17.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.18.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.18.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.19.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.19.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.20.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.20.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.21.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.21.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.22.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.22.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.23.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "transformer.h.23.mlp.gelu_impl has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkecskemetitamass\u001b[0m (\u001b[33mtamaskecskemeti\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240501_182728-vmyotmg2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tamaskecskemeti/huggingface/runs/vmyotmg2' target=\"_blank\">noble-salad-13</a></strong> to <a href='https://wandb.ai/tamaskecskemeti/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tamaskecskemeti/huggingface' target=\"_blank\">https://wandb.ai/tamaskecskemeti/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tamaskecskemeti/huggingface/runs/vmyotmg2' target=\"_blank\">https://wandb.ai/tamaskecskemeti/huggingface/runs/vmyotmg2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [136/136 02:45, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_trained = AutoTokenizer.from_pretrained(\"result_en_1e-05_4\")\n",
        "model_trained = AutoModelForCausalLM.from_pretrained(\"result_en_1e-05_4\").to(device)\n",
        "\n",
        "generated_text = generate_text_from_input(tokenizer_trained, model_trained, text)\n",
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text, generated_text))"
      ],
      "metadata": {
        "id": "-4xpdMZX6A0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2813756-a306-4b80-8dde-eb95f9651a96"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7216494845360825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_trained = AutoTokenizer.from_pretrained(\"result_en_1e-05_8\")\n",
        "model_trained = AutoModelForCausalLM.from_pretrained(\"result_en_1e-05_8\").to(device)\n",
        "\n",
        "generated_text = generate_text_from_input(tokenizer_trained, model_trained, text)\n",
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text, generated_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v_4z4ILYkFt",
        "outputId": "6752a4d0-e58e-409b-a147-4d2e7efc9ffc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6907216494845361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_trained = AutoTokenizer.from_pretrained(\"result_en_2e-05_4\")\n",
        "model_trained = AutoModelForCausalLM.from_pretrained(\"result_en_2e-05_4\").to(device)\n",
        "\n",
        "generated_text = generate_text_from_input(tokenizer_trained, model_trained, text)\n",
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text, generated_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayb209xuYlHb",
        "outputId": "0203940c-19bc-481f-b679-e221a01658b3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6701030927835051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_trained = AutoTokenizer.from_pretrained(\"result_en_2e-05_8\")\n",
        "model_trained = AutoModelForCausalLM.from_pretrained(\"result_en_2e-05_8\").to(device)\n",
        "\n",
        "generated_text = generate_text_from_input(tokenizer_trained, model_trained, text)\n",
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text, generated_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQLf6DREYqHO",
        "outputId": "f509ec0f-8c22-4f7c-c2dc-6b57ecb1b112"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6185567010309279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_trained = AutoTokenizer.from_pretrained(\"result_en_3e-05_4\")\n",
        "model_trained = AutoModelForCausalLM.from_pretrained(\"result_en_3e-05_4\").to(device)\n",
        "\n",
        "generated_text = generate_text_from_input(tokenizer_trained, model_trained, text)\n",
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text, generated_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRu5vgsFYsZf",
        "outputId": "d1b7861b-5937-4263-9e73-81bd6acfbc95"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftecwkM0YSTh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}