{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Bseqc2QxcwxZqQr8ff88BlmvENf7NoyX",
      "authorship_tag": "ABX9TyOnzvLghHQfJLvnOlfwng6B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamaskecskemeti/financial_nlp/blob/main/nlp_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ6rG1F_dZZ_",
        "outputId": "0854500b-fd78-491e-b5ad-e084179a3deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: transformers==4.22.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.22.2)\n",
            "Requirement already satisfied: datasets==2.13.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.13.2)\n",
            "Requirement already satisfied: tensorflow==2.14 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.14.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval==1.2.2->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval==1.2.2->-r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (3.8.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (4.25.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14->-r requirements.txt (line 4)) (0.41.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.22.2->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.22.2->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.22.2->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 1)) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.2->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.2->-r requirements.txt (line 3)) (2023.3.post1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# install required packages written in requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, set_seed, AutoTokenizer\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "WR2g2FVMeASy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "UuUR6mQIlToV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Anc9Bqv_Ylkr",
        "outputId": "86b2b321-1d84-47bc-fdb5-549c2d5fef00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function creator"
      ],
      "metadata": {
        "id": "6S0iKjm9tF7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_from_input(tokenizer, model, input_text):\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "  out = model.generate(input_ids,\n",
        "                     max_new_tokens=100,\n",
        "                     num_beams=5,\n",
        "                     no_repeat_ngram_size=4,\n",
        "                     top_k=50,\n",
        "                     do_sample=True,\n",
        "                     top_p=0.9,\n",
        "                     temperature=1,\n",
        "                     early_stopping=True,\n",
        "                     pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "  out_text = list(map(tokenizer.decode, out))[0]\n",
        "\n",
        "  return out_text"
      ],
      "metadata": {
        "id": "v6N8-cCOouBp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rouge scores for a reference/generated sentence pair\n",
        "# source google seq2seq source code.\n",
        "\n",
        "# supporting function\n",
        "def _split_into_words(sentences):\n",
        "  \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
        "  return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
        "\n",
        "# supporting function\n",
        "def _get_word_ngrams(n, sentences):\n",
        "  \"\"\"Calculates word n-grams for multiple sentences.\n",
        "  \"\"\"\n",
        "  assert len(sentences) > 0\n",
        "  assert n > 0\n",
        "\n",
        "  words = _split_into_words(sentences)\n",
        "  return _get_ngrams(n, words)\n",
        "\n",
        "# supporting function\n",
        "def _get_ngrams(n, text):\n",
        "  \"\"\"Calcualtes n-grams.\n",
        "  Args:\n",
        "    n: which n-grams to calculate\n",
        "    text: An array of tokens\n",
        "  Returns:\n",
        "    A set of n-grams\n",
        "  \"\"\"\n",
        "  ngram_set = set()\n",
        "  text_length = len(text)\n",
        "  max_index_ngram_start = text_length - n\n",
        "  for i in range(max_index_ngram_start + 1):\n",
        "    ngram_set.add(tuple(text[i:i + n]))\n",
        "  return ngram_set\n",
        "\n",
        "def rouge_n(reference_sentences, evaluated_sentences, n=2):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    evaluated_sentences: The sentences that have been picked by the summarizer\n",
        "    reference_sentences: The sentences from the referene set\n",
        "    n: Size of ngram.  Defaults to 2.\n",
        "  Returns:\n",
        "    recall rouge score(float)\n",
        "  Raises:\n",
        "    ValueError: raises exception if a param has len <= 0\n",
        "  \"\"\"\n",
        "  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
        "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
        "\n",
        "  evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
        "  reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
        "  reference_count = len(reference_ngrams)\n",
        "  evaluated_count = len(evaluated_ngrams)\n",
        "\n",
        "  # gets the overlapping ngrams between evaluated and reference\n",
        "  overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
        "  overlapping_count = len(overlapping_ngrams)\n",
        "\n",
        "  # handle edge case. This isn't mathematically correct, but it's good enough\n",
        "  if evaluated_count == 0:\n",
        "    precision = 0.0\n",
        "  else:\n",
        "    precision = overlapping_count / evaluated_count\n",
        "\n",
        "  if reference_count == 0:\n",
        "    recall = 0.0\n",
        "  else:\n",
        "    recall = overlapping_count / reference_count\n",
        "\n",
        "  f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
        "\n",
        "  # just returning recall count in rouge, useful for our purpose\n",
        "  return recall"
      ],
      "metadata": {
        "id": "sS7OpwlOqY2d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# English model"
      ],
      "metadata": {
        "id": "iNtGwxWItwgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some text to test the model\n",
        "# text_en = np.loadtxt(\"train_data_en.txt\")\n",
        "text_en = \"The Turing test, originally called the imitation game by Alan Turing in 1950 is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human.\""
      ],
      "metadata": {
        "id": "fqVEW8t7YlAQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_1_en = AutoTokenizer.from_pretrained(\"ai-forever/mGPT\")\n",
        "model_1_en = GPT2LMHeadModel.from_pretrained(\"ai-forever/mGPT\")\n",
        "tokenizer_2_en = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model_2_en = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "sYgWTwf6pV9V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_1_en = generate_text_from_input(tokenizer_1_en, model_1_en, text_en)\n",
        "generated_text_1_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "gJkhPmPVtPtO",
        "outputId": "20601624-b28b-454e-b1aa-09bcab7fb028"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Turing test, originally called the imitation game by Alan Turing in 1950 is a test of a machine\\'s ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human. It is a form of the Turing test in which a machine is able to demonstrate that it is capable of reasoning in a way similar to that of humans.\\nThe Turing test was first developed by Alan Turing, a British mathematician and computer scientist, in 1950, to test his ability to imitate a human. The test was designed to test Turing\\'s ability to reason in a manner similar to human reasoning. The Turing test was originally called the \"imitation game\" by Turing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_2_en = generate_text_from_input(tokenizer_2_en, model_2_en, text_en)\n",
        "generated_text_2_en"
      ],
      "metadata": {
        "id": "auLo4MlArEkR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0cc83307-8c30-4a31-e05b-a0bbd205555e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Turing test, originally called the imitation game by Alan Turing in 1950 is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human.\\n\\nIn the Turing test, the Turing test is a computer's ability to discriminate between two or more possible outcomes. The Turing test is not a test of human intelligence, but a test of the Turing test.\\n\\nThe Turing test\\n\\nIt is important to note that the Turing test does not determine whether or not a machine is intelligent or not. Rather, it determines whether or not the Turing test can be used to determine whether a machine is smarter or not.\\n\\nTuring is\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the reference text is used to evaluate the generated text\n",
        "ref_text_en = \"The Turing test, originally called the imitation game by Alan Turing in 1950 is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation was a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel, such as a computer keyboard and screen, so the result would not depend on the machine's ability to render words as speech. If the evaluator could not reliably tell the machine from the human, the machine would be said to have passed the test. The test results would not depend on the machine's ability to give correct answers to questions, only on how closely its answers resembled those a human would give. Since the Turing test is a test of indistinguishability in performance capacity, the verbal version generalizes naturally to all of human performance capacity, verbal as well as nonverbal (robotic).\""
      ],
      "metadata": {
        "id": "L5_mJY7ktoPs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_en, generated_text_1_en))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T0jtFgw-9ZY",
        "outputId": "c15fc4d5-f290-49ca-d14d-4e36d43965f2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6260162601626016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_en, generated_text_2_en))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7pvusaSFd_v",
        "outputId": "18c09b96-9d98-4348-9487-48076daa2e2e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6056910569105691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hungarian model"
      ],
      "metadata": {
        "id": "05dOnZW2NHWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some text in hungarian to test the model\n",
        "text_hu = \"A Turing-teszt, amelyet eredetileg Alan Turing 1950-ben imitációs játéknak nevezett el, egy olyan teszt, amely azt vizsgálja, hogy egy gép képes-e az emberi viselkedéssel egyenértékű vagy attól megkülönböztethetetlen intelligens viselkedést tanúsítani.\"\n"
      ],
      "metadata": {
        "id": "p7_Sc7BVrpJ6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_1_hu = generate_text_from_input(tokenizer_1_en, model_1_en, text_hu)"
      ],
      "metadata": {
        "id": "MnQyOhikfpUq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_1_hu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "CQ4YWZZ9Rr-V",
        "outputId": "65afc908-c53a-4820-d8d3-46827767cc86"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A Turing-teszt, amelyet eredetileg Alan Turing 1950-ben imitációs játéknak nevezett el, egy olyan teszt, amely azt vizsgálja, hogy egy gép képes-e az emberi viselkedéssel egyenértékű vagy attól megkülönböztethetetlen intelligens viselkedést tanúsítani. A tesztet úgy tervezték, hogy a Turing-teszttel ellentétben ne tegyék lehetővé, hogy az emberi agy ne tudjon azonosulni a gépekkel.\\nA tesztet az 1950-es évek elején végezték el, és a tesztet olyan számítógépeken végezték, amelyek nem rendelkeztek a Turing-testekkel. Ezek a számítógépek a következők voltak:\\nA tesztrendszer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_2_hu = AutoTokenizer.from_pretrained(\"NYTK/PULI-GPT-2\")\n",
        "model_2_hu = GPT2LMHeadModel.from_pretrained(\"NYTK/PULI-GPT-2\")\n",
        "\n",
        "generated_text_2_hu = generate_text_from_input(tokenizer_2_hu, model_2_hu, text_hu)"
      ],
      "metadata": {
        "id": "QTHSQs2Sr53q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_2_hu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "o-2gfCG8RsfN",
        "outputId": "f9eadac7-9024-4a4d-d7fd-d5a40b577ad4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A Turing-teszt, amelyet eredetileg Alan Turing 1950-ben imitációs játéknak nevezett el, egy olyan teszt, amely azt vizsgálja, hogy egy gép képes-e az emberi viselkedéssel egyenértékű vagy attól megkülönböztethetetlen intelligens viselkedést tanúsítani. A Turing-tesztet az 1950-es években kezdték el kidolgozni, és azóta is széles körben alkalmazzák.\\nA Turing teszt lényege, hogy a gép képes legyen az emberi viselkedéshez hasonló vagy attól eltérő intelligenciát tanúsítani. Ez azt jelenti, hogy a teszt során a gépnek képesnek kell lennie az emberi viselkedésre hasonló vagy attól különböző viselkedést tanúsítani, függetlenül attól, hogy az adott gép képes volt-e arra, hogy az emberi viselkedéstől eltérő viselkedést tanúsítson'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the reference text is used to evaluate the generated text\n",
        "ref_text_hu = \"A Turing-teszt, amelyet eredetileg Alan Turing 1950-ben imitációs játéknak nevezett el, egy olyan teszt, amely azt vizsgálja, hogy egy gép képes-e az emberi viselkedéssel egyenértékű vagy attól megkülönböztethetetlen intelligens viselkedést tanúsítani. Turing azt javasolta, hogy egy emberi értékelő értékelje az ember és egy emberhez hasonló válaszok generálására tervezett gép közötti természetes nyelvű beszélgetéseket. Az értékelő tisztában lenne azzal, hogy a két beszélgetőpartner közül az egyik egy gép, és a résztvevők el lennének választva egymástól. A beszélgetés kizárólag szöveges csatornára korlátozódna, például számítógépes billentyűzetre és képernyőre, így az eredmény nem függne attól, hogy a gép képes-e a szavakat beszédként megjeleníteni. Ha az értékelő nem tudná megbízhatóan megkülönböztetni a gépet az embertől, akkor a gép átmenne a teszten. A teszt eredménye nem függne attól, hogy a gép képes-e helyes válaszokat adni a kérdésekre, csak attól, hogy a válaszai mennyire hasonlítanak az emberi válaszokhoz. Mivel a Turing-teszt a teljesítőképesség megkülönböztethetetlenségének tesztje, a verbális változat természetesen általánosítható az emberi teljesítőképesség egészére, a verbális és a nem verbális (robotikus) teljesítőképességre egyaránt.\""
      ],
      "metadata": {
        "id": "5gcijuhJt5YE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_hu, generated_text_1_hu))"
      ],
      "metadata": {
        "id": "gazEI81KA6QS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6328d903-9398-4b38-8cb2-7af3408747f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5828025477707006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_hu, generated_text_2_hu))"
      ],
      "metadata": {
        "id": "YfgE3W0vEazT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73677261-31fb-4e61-be05-31b675ad14cd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6273885350318471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune models"
      ],
      "metadata": {
        "id": "T5TFNXLXuFgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "  dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer):\n",
        "  data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "  return data_collator\n",
        "\n",
        "\n",
        "def train(input_path,\n",
        "          model_name,\n",
        "          output_path,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(input_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_path)\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "  model.save_pretrained(output_path)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_path,\n",
        "          overwrite_output_dir=False,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "metadata": {
        "id": "4Pb-MtUF3P3A"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path_en = \"train_data_en.txt\"\n",
        "output_path_en = \"result_en\"\n",
        "model_name_en = \"gpt2\"\n",
        "\n",
        "input_path_hu = \"train_data_hu.txt\"\n",
        "output_path_hu = \"result_hu\"\n",
        "model_name_hu = \"NYTK/PULI-GPT-2\"\n",
        "per_device_train_batch_size = 6\n",
        "num_train_epochs = 256\n",
        "save_steps = 250"
      ],
      "metadata": {
        "id": "n8YiY68FOQs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    input_path=input_path_en,\n",
        "    model_name=model_name_en,\n",
        "    output_path=output_path_en,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ],
      "metadata": {
        "id": "uY8V2-dzcHne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    input_path=input_path_hu,\n",
        "    model_name=model_name_hu,\n",
        "    output_dir=output_path_hu,\n",
        "    overwrite_output_dir=False,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ],
      "metadata": {
        "id": "DYWjPrFavH5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_trained_en = AutoTokenizer.from_pretrained(output_path_en)\n",
        "model_trained_en = GPT2LMHeadModel.from_pretrained(output_path_en)\n",
        "\n",
        "generated_text_en = generate_text_from_input(tokenizer_trained_en, model_trained_en, text_en)"
      ],
      "metadata": {
        "id": "SHsmJyCPcJJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "VDh8zEnr57Qp",
        "outputId": "c8235e72-a437-40c0-ec08-d58a9b6311f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The economics is:\\na\\nprice\\ntheoretically\\nconceivable\\nwould be\\na\\nthing\\n.\\nFrom\\nsure\\n,\\nthat\\non\\nApple\\nvery\\nquickly\\nout\\nmust\\nfind\\nsomething\\na\\n29\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_trained_hu = AutoTokenizer.from_pretrained(output_path_hu)\n",
        "model_trained_hu = GPT2LMHeadModel.from_pretrained(output_path_hu)\n",
        "\n",
        "generated_text_hu = generate_text_from_input(tokenizer_trained_hu, model_trained_hu, text_hu)"
      ],
      "metadata": {
        "id": "uISXw4kwxjKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_en, generated_text_en))"
      ],
      "metadata": {
        "id": "mHz2pz91QRfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_hu, generated_text_hu))"
      ],
      "metadata": {
        "id": "gP0gCivDyYm0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}