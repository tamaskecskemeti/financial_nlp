{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Bseqc2QxcwxZqQr8ff88BlmvENf7NoyX",
      "authorship_tag": "ABX9TyOTpPnsKe3SKDrSaNLiUjwg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamaskecskemeti/financial_nlp/blob/main/nlp_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ6rG1F_dZZ_",
        "outputId": "b06a3c8b-9543-4a25-c685-39d60936a8d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: transformers==4.22.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.22.2)\n",
            "Requirement already satisfied: datasets==2.13.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.13.2)\n",
            "Requirement already satisfied: tensorflow==2.14 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.14.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval==1.2.2->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval==1.2.2->-r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.22.2->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2->-r requirements.txt (line 3)) (3.8.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14->-r requirements.txt (line 4)) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14->-r requirements.txt (line 4)) (0.41.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.22.2->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.22.2->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.22.2->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 1)) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.2->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.2->-r requirements.txt (line 3)) (2023.3.post1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14->-r requirements.txt (line 4)) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# install required packages written in requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, set_seed, AutoTokenizer\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "WR2g2FVMeASy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "UuUR6mQIlToV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Anc9Bqv_Ylkr",
        "outputId": "594cc26c-7351-4617-925d-16e4f8ee3bb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function creator"
      ],
      "metadata": {
        "id": "6S0iKjm9tF7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_from_input(tokenizer, model, input_text):\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  out = model.generate(input_ids,\n",
        "                     max_new_tokens=100,\n",
        "                     num_beams=5,\n",
        "                     no_repeat_ngram_size=4,\n",
        "                     top_k=50,\n",
        "                     do_sample=True,\n",
        "                     top_p=0.9,\n",
        "                     temperature=1,\n",
        "                     early_stopping=True,\n",
        "                     pad_token_id=tokenizer.eos_token_id).to(device)\n",
        "\n",
        "  out_text = list(map(tokenizer.decode, out))[0]\n",
        "\n",
        "  return out_text"
      ],
      "metadata": {
        "id": "v6N8-cCOouBp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rouge scores for a reference/generated sentence pair\n",
        "# source google seq2seq source code.\n",
        "\n",
        "# supporting function\n",
        "def _split_into_words(sentences):\n",
        "  \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
        "  return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
        "\n",
        "# supporting function\n",
        "def _get_word_ngrams(n, sentences):\n",
        "  \"\"\"Calculates word n-grams for multiple sentences.\n",
        "  \"\"\"\n",
        "  assert len(sentences) > 0\n",
        "  assert n > 0\n",
        "\n",
        "  words = _split_into_words(sentences)\n",
        "  return _get_ngrams(n, words)\n",
        "\n",
        "# supporting function\n",
        "def _get_ngrams(n, text):\n",
        "  \"\"\"Calcualtes n-grams.\n",
        "  Args:\n",
        "    n: which n-grams to calculate\n",
        "    text: An array of tokens\n",
        "  Returns:\n",
        "    A set of n-grams\n",
        "  \"\"\"\n",
        "  ngram_set = set()\n",
        "  text_length = len(text)\n",
        "  max_index_ngram_start = text_length - n\n",
        "  for i in range(max_index_ngram_start + 1):\n",
        "    ngram_set.add(tuple(text[i:i + n]))\n",
        "  return ngram_set\n",
        "\n",
        "def rouge_n(reference_sentences, evaluated_sentences, n=2):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    evaluated_sentences: The sentences that have been picked by the summarizer\n",
        "    reference_sentences: The sentences from the referene set\n",
        "    n: Size of ngram.  Defaults to 2.\n",
        "  Returns:\n",
        "    recall rouge score(float)\n",
        "  Raises:\n",
        "    ValueError: raises exception if a param has len <= 0\n",
        "  \"\"\"\n",
        "  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
        "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
        "\n",
        "  evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
        "  reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
        "  reference_count = len(reference_ngrams)\n",
        "  evaluated_count = len(evaluated_ngrams)\n",
        "\n",
        "  # gets the overlapping ngrams between evaluated and reference\n",
        "  overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
        "  overlapping_count = len(overlapping_ngrams)\n",
        "\n",
        "  # handle edge case. This isn't mathematically correct, but it's good enough\n",
        "  if evaluated_count == 0:\n",
        "    precision = 0.0\n",
        "  else:\n",
        "    precision = overlapping_count / evaluated_count\n",
        "\n",
        "  if reference_count == 0:\n",
        "    recall = 0.0\n",
        "  else:\n",
        "    recall = overlapping_count / reference_count\n",
        "\n",
        "  f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
        "\n",
        "  # just returning recall count in rouge, useful for our purpose\n",
        "  return recall"
      ],
      "metadata": {
        "id": "sS7OpwlOqY2d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# English model"
      ],
      "metadata": {
        "id": "iNtGwxWItwgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some text to test the model\n",
        "text_en = Path(\"generate_text_en.txt\").read_text()"
      ],
      "metadata": {
        "id": "fqVEW8t7YlAQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_1_en = AutoTokenizer.from_pretrained(\"ai-forever/mGPT\")\n",
        "model_1_en = GPT2LMHeadModel.from_pretrained(\"ai-forever/mGPT\").to(device)\n",
        "tokenizer_2_en = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model_2_en = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)"
      ],
      "metadata": {
        "id": "sYgWTwf6pV9V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_1_en = generate_text_from_input(tokenizer_1_en, model_1_en, text_en)\n",
        "generated_text_1_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "gJkhPmPVtPtO",
        "outputId": "1d8045db-86db-4b6b-ebbd-4b6d5cf63eb0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One of the biggest names in Silicon Valley is placing a moonshot bet on bitcoin BTCUSD, +0.72%. \\nFounders Fund, the venture-capital firm co-founded by Peter Thiel, has amassed hundreds of millions of dollars of the volatile cryptocurrency, people familiar with the matter said.\\n“Bitcoin is a great investment,” Thiel told CNBC. “It’s a great investment, but it’s not a good investment. It’s a good investment, but you’re not going to get a lot of money out of it.”\\nBitcoin’s volatility has been a problem for investors in the cryptocurrencies since the beginning of the year, according to Thiel.\\n“I don’t think that’s'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_2_en = generate_text_from_input(tokenizer_2_en, model_2_en, text_en)\n",
        "generated_text_2_en"
      ],
      "metadata": {
        "id": "auLo4MlArEkR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "8b419c90-a051-4a42-a3e9-9b56e2869563"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One of the biggest names in Silicon Valley is placing a moonshot bet on bitcoin BTCUSD, +0.72%. \\nFounders Fund, the venture-capital firm co-founded by Peter Thiel, has amassed hundreds of millions of dollars of the volatile cryptocurrency, people familiar with the matter said. \\n\\nThe fund, which has raised more than $100 million in funding, has raised $2.5 billion in the past two years, according to the people, who spoke on condition of anonymity because they were not authorized to discuss the matter publicly.\\n\\nIn a statement, the fund said: \"We are pleased to announce that we have reached a critical milestone in the development of Bitcoin. We are now at the point where we are ready to invest in the next phase of the project'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the reference text is used to evaluate the generated text\n",
        "ref_text_en = Path(\"reference_text_en.txt\").read_text()"
      ],
      "metadata": {
        "id": "L5_mJY7ktoPs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_en, generated_text_1_en))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T0jtFgw-9ZY",
        "outputId": "e8e42811-9ffc-4424-b807-0f50b225dd57"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6632302405498282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_en, generated_text_2_en))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7pvusaSFd_v",
        "outputId": "107b1d1d-53d5-4793-9fec-c80f95089e11"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7250859106529209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hungarian model"
      ],
      "metadata": {
        "id": "05dOnZW2NHWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some text in hungarian to test the model\n",
        "text_hu = Path(\"generate_text_hu.txt\").read_text()\n"
      ],
      "metadata": {
        "id": "p7_Sc7BVrpJ6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_1_hu = generate_text_from_input(tokenizer_1_en, model_1_en, text_hu)"
      ],
      "metadata": {
        "id": "MnQyOhikfpUq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_1_hu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "CQ4YWZZ9Rr-V",
        "outputId": "ff7f6c01-f9c5-4f33-fa14-b5a13fcfa0b1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A Szilícium-völgy egyik legnagyobb neve a bitcoin-ra tesz, ami +0,72%-kal növekedett. \\nA Founders Fund, a Peter Thiel által társalapított kockázati tőkecég több százmillió dollárt halmozott fel az ingatag kriptopénzből, mondták az ügyet ismerő személyek.\\nA bitcoin árfolyama az elmúlt hetekben megduplázódott.\\nAz elmúlt napokban a kriptovaluták árfolyama folyamatosan emelkedett.\\nA New York-i tőzsdén a Bitcoin (BTC) árfolyama 0,7 százalékkal emelkedett hétfőn.\\nA BTC árfolyama hétfőn 0,8 százalékot esett'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_2_hu = AutoTokenizer.from_pretrained(\"NYTK/PULI-GPT-2\")\n",
        "model_2_hu = GPT2LMHeadModel.from_pretrained(\"NYTK/PULI-GPT-2\").to(device)\n",
        "\n",
        "generated_text_2_hu = generate_text_from_input(tokenizer_2_hu, model_2_hu, text_hu)"
      ],
      "metadata": {
        "id": "QTHSQs2Sr53q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_2_hu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "o-2gfCG8RsfN",
        "outputId": "def94486-aa09-4249-88ec-6709d6d170f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A Szilícium-völgy egyik legnagyobb neve a bitcoin-ra tesz, ami +0,72%-kal növekedett. \\nA Founders Fund, a Peter Thiel által társalapított kockázati tőkecég több százmillió dollárt halmozott fel az ingatag kriptopénzből, mondták az ügyet ismerő személyek.\\nA CNBC-nek nyilatkozó szakértők szerint a kriptopénz nem is annyira kriptopénz, mint inkább egy olyan eszköz, amely a decentralizáltság és a decentralizált döntéshozatal révén képes alkalmazkodni a változó körülményekhez.\\nA kriptopénz egy olyan eszközzé válhat, amely képes alkalmazkodni a változásokhoz, és képes alkalmazkodni a megváltozott körülményekhez.<|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the reference text is used to evaluate the generated text\n",
        "ref_text_hu = Path(\"reference_text_hu.txt\").read_text()"
      ],
      "metadata": {
        "id": "5gcijuhJt5YE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_hu, generated_text_1_hu))"
      ],
      "metadata": {
        "id": "gazEI81KA6QS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f899c7-ebc7-4cf8-9056-11ce440e0208"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5847457627118644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_hu, generated_text_2_hu))"
      ],
      "metadata": {
        "id": "YfgE3W0vEazT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c42bb22a-c422-4694-9285-85ca90625ddb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.632768361581921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune models"
      ],
      "metadata": {
        "id": "T5TFNXLXuFgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "  dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer):\n",
        "  data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "  return data_collator\n",
        "\n",
        "\n",
        "def train(input_path,\n",
        "          model_name,\n",
        "          output_path,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(input_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_path)\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "  model.save_pretrained(output_path)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_path,\n",
        "          overwrite_output_dir=False,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "metadata": {
        "id": "4Pb-MtUF3P3A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path_en = \"train_text_en_coded.txt\"\n",
        "output_path_en = \"result_en\"\n",
        "model_name_en = \"gpt2\"\n",
        "\n",
        "input_path_hu = \"train_text_hu.txt\"\n",
        "output_path_hu = \"result_hu\"\n",
        "model_name_hu = \"NYTK/PULI-GPT-2\"\n",
        "per_device_train_batch_size = 6\n",
        "num_train_epochs = 64\n",
        "save_steps = 250"
      ],
      "metadata": {
        "id": "n8YiY68FOQs7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    input_path=input_path_en,\n",
        "    model_name=model_name_en,\n",
        "    output_path=output_path_en,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ],
      "metadata": {
        "id": "aFDzT5P3-GqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    input_path=input_path_hu,\n",
        "    model_name=model_name_hu,\n",
        "    output_dir=output_path_hu,\n",
        "    overwrite_output_dir=False,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ],
      "metadata": {
        "id": "DYWjPrFavH5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_trained_en = AutoTokenizer.from_pretrained(output_path_en)\n",
        "model_trained_en = GPT2LMHeadModel.from_pretrained(output_path_en)\n",
        "\n",
        "generated_text_en = generate_text_from_input(tokenizer_trained_en, model_trained_en, text_en)"
      ],
      "metadata": {
        "id": "SHsmJyCPcJJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "VDh8zEnr57Qp",
        "outputId": "c8235e72-a437-40c0-ec08-d58a9b6311f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The economics is:\\na\\nprice\\ntheoretically\\nconceivable\\nwould be\\na\\nthing\\n.\\nFrom\\nsure\\n,\\nthat\\non\\nApple\\nvery\\nquickly\\nout\\nmust\\nfind\\nsomething\\na\\n29\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_trained_hu = AutoTokenizer.from_pretrained(output_path_hu)\n",
        "model_trained_hu = GPT2LMHeadModel.from_pretrained(output_path_hu)\n",
        "\n",
        "generated_text_hu = generate_text_from_input(tokenizer_trained_hu, model_trained_hu, text_hu)"
      ],
      "metadata": {
        "id": "uISXw4kwxjKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_en, generated_text_en))"
      ],
      "metadata": {
        "id": "mHz2pz91QRfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rouge value can be between 0 and 1. The higher value is better\n",
        "print(rouge_n(ref_text_hu, generated_text_hu))"
      ],
      "metadata": {
        "id": "gP0gCivDyYm0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}